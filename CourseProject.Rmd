---
title: "Course project for machine learning"
author: "Kristina Juhlin"
date: "11 november 2015"
output: html_document
---
## Project  task
The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

## Introduction 
The aim of this project is to classify a number of performed excercises as correctly or incorrectly executed. If incorrectly executed, I will identify in which if four possible ways they are faulting. 

```{r libraries, echo=FALSE, message=FALSE}
library(caret)
library(plyr)
library(dplyr)
library(gridExtra)
library(doParallel)
library(gbm)
library(randomForest)
library(foreach)
library(knitr)
library(gridExtra)
```

## Data 
This project uses the weight lifting excercise dataset collected by [Vellosso et al] (http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201) which includes measurements of arm, forearm, hip and dumbbell movements while performing a weightlifting excercise. The excercises are classified as correctly (class A) performed or wrongly performed in one of the following ways: throwing the elbows to the front (class B), lifting the dumbbell only halfway (class C), lowering the dumbbell only halfway (class D) and throwing the hips to the front (class E).

```{r loadData, echo=FALSE, message=FALSE}
if (!file.exists("pml-training.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","pml-training.csv")
}
pml_data<- read.csv("pml-training.csv", header = TRUE, na.strings = c("","NA"))

levels(pml_data$classe)<-c("Correct", "ElbowsFront", "LiftHalf", "LowHalf", "HipsFront")
```

## Creating the predictive model

### Test and training set
I divided the 19622 samples into a training set (60% of samples) and a testing set (40% of samples). Exploratory analysis was then performed on the samples in the training set. 

```{r createSets, echo=TRUE, message=FALSE}
# Divide data into training and testing set 
set.seed(1000)
inTrain = createDataPartition(pml_data$classe, p = 0.6)[[1]]

training = pml_data[ inTrain,]
testing  = pml_data[-inTrain,]
```

### Exploratory analysis 


```{r explore, echo=TRUE, results=FALSE}
names(training)
summary(training)
str(training)
```

```{r windows, echo=FALSE, cache=TRUE, fig.height=4, fig.width=8}

# Number of time windows for each participant and event type
t <- count(training, num_window, classe, user_name);

# Number of new windows for each user and classe
t2 <- training %>% filter(new_window=="yes") %>% count(classe, user_name);

g1<-ggplot(data=t2, aes(x=classe,y=n, fill=classe))+geom_boxplot()+geom_point()
g2<-ggplot(data=t, aes(x=classe,y=n, fill=classe))+geom_boxplot()+geom_jitter()

grid.arrange(g1,g2, nrow=1)
```


```{r carlitos, echo=FALSE, cache=TRUE}

# look at one activity for one participant

carlitos <- filter(training, user_name=="carlitos" & classe=="A");
head(select(carlitos,raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window),10)

# Look at one window 
carlitos %>% select(raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window) %>% filter(num_window==12)
```


Remove columns with information about the time window since this will not be used for building the model. Also remove the column containing the user name. 

```{r processData_part1, echo=FALSE, cache=TRUE, message=FALSE}
# exclude user name and time window columns
removeColumns <- function(data){
    data <- data[,-grep("timestamp",names(data))]
    data <- data[,-grep("window",names(data))]
    data <- data[,-grep("user_name",names(data))]
    data <- data[,-grep("X",names(data))] 
}
training <- removeColumns(training)

# Store classe in separate variable 
training.class <- training$classe
training$classe<-NULL;
```

Look for missing values in the data. Some oclumns have almost only NAs. A look at these columns tells me that these measurements are the ones that are reported only ones per time series, for example the standard deviation of a particular measurement. Since I am not using the time series data and will no know which time series the values in the test data are from, I will exclude these values from the dataset. 

```{r NAs, echo=TRUE, cache=TRUE, message=FALSE, fig.width=4}
# Identify missing values
missing <- apply(training, 2, function(x) sum(is.na(x)));
hist(missing, col="steelblue", main="Frequency of missing values")

# either no missing or almost all missing => no point in imputing values
names(training[,missing!=0])

# remove columns with NAs
training<- training[,missing==0];
```

Continue exploring the remaining data. Are there highly correlated covariates? 

```{r cor, echo=FALSE, cache=TRUE, message=FALSE}

# Check correlation between all numeric variables 
numerics <- sapply(training, is.numeric)
correlationMatrix <- cor(training[,numerics])
correlated <- findCorrelation(correlationMatrix, cutoff=0.9)

head(correlationMatrix[,correlated])

# delete feature with correlation>=0.9
# they provide little extra information and slow down model making 
training <- training[,-correlated]

# turn all remaining variables of training into numeric (prevents later errors in prediction function)
training<-sapply(training,as.numeric);
```

The training data is now fully processed, let's process the testing data in the same way. I will remove columns with user name and time stamp information, and columns with missing values and correlayed variables (based on column names identified from training data).

```{r processTestData, echo=FALSE, cache=TRUE, message=FALSE}
# Transform test data in same way as training data
testing.class <- testing$classe
testing$classe<-NULL;
testing <- removeColumns(testing)
testing <- testing[,missing==0];
testing <- testing[,-correlated]
testing<-sapply(testing,as.numeric);
```


To do:

### Explore 

- Number of NAs per feature
- Which features seem to be most important based on documentation?

### Feature selection/engineering 

- Normalizing features? (centering and scaling, box-cox)
- Imputing missing values?
- Identify outliers?
- Strongly correlated values?

- PCA analysis? 
- Use user_name as predictor? Or to standardize? 

### Choice of algorithm 
Not a linerar relationship between predictors and outcome => not regression. 

- Decision trees? 
- knn?

### Fitting model 

- cross validation
- make tree model using rpart to understand data better? 



```{r standardGBM, echo=FALSE, cache=TRUE, message=FALSE}

# fit standard gbm models 
# use multiple cores
registerDoParallel(cores=2)

# Use 5-fold cross validation instead of boosting to save time and prevent bias 
fit_pca    <- train(training,training.class, method="gbm", 
                    trControl = trainControl(method = "cv", number = 5), 
                    preProcess = "pca")
fit_org<- train(training, training.class, method="gbm", 
                trControl = trainControl(method = "cv", number = 5))
fit_scale<- train(training, training.class, method="gbm", 
                  trControl = trainControl(method = "cv", number = 5), 
                  preProcess = c("center","scale"))

fit_rf <- train(training, training.class, method="rf", 
                trControl = trainControl(method = "cv", number = 5))

# Look at insample accuraccy 
kable(fit_org$results, caption="GBM model with standard parameters")
kable(fit_pca$results, caption="GBM model using principal components")
kable(fit_scale$results, caption="GBM model using scaling")
kable(fit_rf$results, caption="Random forest model with standard parameters")
# Random forests look really good

# Why does PCA make everything so much worse?
# plot top 5 variables chosen by non_pca method
head(summary(fit_org),10)

```

```{r tuneModels, echo=FALSE, cache=TRUE}
# Fit several random tree models using different input parameters, use cross valdiation to determine quality of models

# 1. How many trees do I need to fit? 
gbmGrid <-  expand.grid(interaction.depth = 3,
                        n.trees = c(100,200,300,400,500),
                        shrinkage = 0.1,
                        n.minobsinnode = 10)

fit_nTrees<- train(training, training.class, 
                 method="gbm", 
                 trControl = trainControl(method = "cv", number = 5),
                 tuneGrid = gbmGrid, 
                 verbose=FALSE)

# 2. What is the most appropriate interaction depth?
gbmGrid <-  expand.grid(interaction.depth = c(1,2,3,4,5),
                        n.trees = c(200),
                        shrinkage = 0.1,
                        n.minobsinnode = 10)

fit_depth<- train(training, training.class, 
                 method="gbm", 
                 trControl = trainControl(method = "cv", number = 5),
                 tuneGrid = gbmGrid, 
                 verbose=FALSE)

# 3. Best value for shrinkage?
gbmGrid <-  expand.grid(interaction.depth = 3,
                        n.trees = 200,
                        shrinkage = c(0.25, 0.1, 0.05),
                        n.minobsinnode = 10)

fit_shrink<- train(training, training.class, 
                 method="gbm", 
                 trControl = trainControl(method = "cv", number = 5),
                 tuneGrid = gbmGrid, 
                 verbose=FALSE)


# 3b. Best value for shrinkage?
gbmGrid <-  expand.grid(interaction.depth = 3,
                        n.trees = 500,
                        shrinkage = c(0.25, 0.1, 0.05),
                        n.minobsinnode = 10)

fit_shrink_n500<- train(training, training.class, 
                 method="gbm", 
                 trControl = trainControl(method = "cv", number = 5),
                 tuneGrid = gbmGrid, 
                 verbose=FALSE)


# 4. Minimum node size: 10 
gbmGrid <-  expand.grid(interaction.depth = 3,
                        n.trees = 200,
                        shrinkage = c(0.1),
                        n.minobsinnode = c(5,10,20))

fit_node<- train(training, training.class, 
                 method="gbm", 
                 trControl = trainControl(method = "cv", number = 5),
                 tuneGrid = gbmGrid)

# Plot results from all fits
g1 <- ggplot(fit_depth)
g2 <- ggplot(fit_node)
g3 <- ggplot(fit_shrink_n500)
g4 <- ggplot(fit_nTrees)
library(gridExtra)

grid.arrange(g1,g2,g3,g4)

# 5. Final model
gbmGrid <-  expand.grid(interaction.depth = 5,
                        n.trees = 500,
                        shrinkage = 0.25,
                        n.minobsinnode = 10)

fit_gmb_final<- train(training, training.class, 
                 method="gbm", 
                 trControl = trainControl(method = "cv", number = 5),
                 verbose=FALSE,
                 tuneGrid = gbmGrid)

fit_rf <- train(training, training.class, method="rf", 
                trControl = trainControl(method = "cv", number = 5),
                allowParallel=TRUE,
                prox=TRUE)

```

## Selecting final model

The model with the highest accuracy from cross-validation is the GBM model. When I apply it to the dataset reserved for testing it gives an accuraccy of X. Based on this, I estimate the out-of-sample error to be: 

```{r comparePerformance, echo=FALSE, cache=TRUE}
fit_gmb_final$results
fit_rf$results

fit_rf$finalModel
confusionMatrix(fit_gmb_final)
confusionMatrix(fit_rf)

# Test both models on test dataset

pred_gbm <- predict(fit_gmb_final, newdata=testing)
pred_rf  <- predict(fit_rf, newdata=testing)

confusionMatrix(pred_gbm, testing.class)
confusionMatrix(pred_rf, testing.class)

mean(pred_rf==testing.class)
mean(pred_gbm==testing.class)
```

## Applying to test cases

```{r loadTest, echo=FALSE, cache=TRUE, message=FALSE}
if (!file.exists("pml-training.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","pml-testing.csv")
}
tests<- read.csv("pml-testing.csv", header = TRUE, na.strings = c("","NA"))
tests.id <- tests$problem_id;
tests$problem_id <- NULL;

tests <- removeColumns(tests)
tests <- tests[,missing==0]
tests <- tests[,-correlated]
tests <- sapply(tests,as.numeric);

pred_submission<- predict(fit_gmb_final, newdata=tests)
levels(pred_submission)<- c("A", "B", "C", "D", "E")
```

```{r writeToFile, echo=FALSE, cache=TRUE, message=FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(pred_submission)
```
